# knowledge.py
# Personal Knowledge Base / RAG over your notes & PDFs
# OFFLINE VERSION: uses sentence-transformers instead of OpenAI embeddings.

import os
import json
import re

import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer

# Folder containing your study material
DOCS_DIR = "knowledge_docs"
INDEX_FILE = "knowledge_index.json"

# Sentence-transformers model (offline, no API)
MODEL_NAME = "all-MiniLM-L6-v2"
_model = None


def _get_model():
    """
    Lazy-load the sentence-transformers model once.
    """
    global _model
    if _model is None:
        print(f"[KB] Loading local embedding model: {MODEL_NAME}")
        _model = SentenceTransformer(MODEL_NAME)
        print("[KB] Model loaded.")
    return _model


def _ensure_docs_dir():
    if not os.path.exists(DOCS_DIR):
        os.makedirs(DOCS_DIR, exist_ok=True)


def _list_documents():
    """
    List all supported files inside knowledge_docs/.
    Supported: .pdf, .txt, .md
    """
    _ensure_docs_dir()
    exts = (".pdf", ".txt", ".md")
    files = []
    for root, dirs, fs in os.walk(DOCS_DIR):
        for f in fs:
            if f.lower().endswith(exts):
                files.append(os.path.join(root, f))
    return files


def _load_pdf(path: str) -> str:
    try:
        with open(path, "rb") as f:
            reader = PdfReader(f)
            texts = []
            for page in reader.pages:
                t = page.extract_text() or ""
                texts.append(t)
        return "\n".join(texts)
    except Exception as e:
        print("[KB] PDF read error:", path, e)
        return ""


def _load_textfile(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()
    except Exception as e:
        print("[KB] Text read error:", path, e)
        return ""


def _chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200):
    """
    Very simple chunking: fixed-size windows with overlap.
    """
    text = text.strip()
    if not text:
        return []

    chunks = []
    start = 0
    length = len(text)

    while start < length:
        end = min(start + chunk_size, length)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        if end == length:
            break
        start = end - overlap

    return chunks


def _cosine_similarity(a, b):
    a = np.array(a, dtype=float)
    b = np.array(b, dtype=float)
    dot = float(np.dot(a, b))
    na = float(np.linalg.norm(a))
    nb = float(np.linalg.norm(b))
    if na == 0 or nb == 0:
        return 0.0
    return dot / (na * nb)


def rebuild_knowledge_base(openai_client=None):
    """
    Re-scan all docs in knowledge_docs/ and rebuild vector index.
    OFFLINE: uses local sentence-transformers model; ignores openai_client.
    """
    files = _list_documents()
    if not files:
        return (
            "I did not find any files in the 'knowledge_docs' folder, sir. "
            "Place your PDFs or notes there and say reload my knowledge again."
        )

    print("[KB] Rebuilding knowledge index from files:", files)

    all_entries = []

    for path in files:
        ext = os.path.splitext(path)[1].lower()
        if ext == ".pdf":
            raw = _load_pdf(path)
        else:
            raw = _load_textfile(path)

        if not raw.strip():
            continue

        chunks = _chunk_text(raw, chunk_size=1200, overlap=200)
        print(f"[KB] {path}: {len(chunks)} chunks")
        for ch in chunks:
            all_entries.append(
                {
                    "source": path,
                    "text": ch,
                }
            )

    if not all_entries:
        return "I could not extract any text from your documents, sir."

    # Compute embeddings in batches using local model
    model = _get_model()
    texts = [e["text"] for e in all_entries]
    print(f"[KB] Creating embeddings for {len(texts)} chunks (offline model)...")

    try:
        embeddings = model.encode(texts, batch_size=16, show_progress_bar=True)
    except TypeError:
        # Some old versions don't support show_progress_bar
        embeddings = model.encode(texts, batch_size=16)

    for entry, emb in zip(all_entries, embeddings):
        entry["embedding"] = emb.tolist()

    data = {"entries": all_entries}
    try:
        with open(INDEX_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f)
    except Exception as e:
        print("[KB] Failed to save index:", e)
        return "I tried to save the knowledge index, but something went wrong, sir."

    return f"I have indexed {len(all_entries)} chunks from {len(files)} files in your knowledge base, sir."


def _load_index():
    if not os.path.exists(INDEX_FILE):
        return None
    try:
        with open(INDEX_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("entries", [])
    except Exception as e:
        print("[KB] Failed to load index:", e)
        return None


def answer_from_knowledge(question: str, openai_client=None):
    """
    Use the prebuilt index + GPT (from main.py) to answer from personal notes.
    OFFLINE embeddings for retrieval, GPT for generation is still done in main.py.
    """
    entries = _load_index()
    if not entries:
        return (
            "I do not have any indexed notes yet, sir. "
            "Put some PDFs or text files into 'knowledge_docs' and say reload my knowledge."
        )

    # Embed the question using local model
    model = _get_model()
    try:
        q_emb = model.encode([question])[0]
    except Exception as e:
        print("[KB] Question embedding error:", e)
        return "I tried to search your knowledge base, but the local embedding step failed, sir."

    # Score each chunk
    scored = []
    for e in entries:
        emb = e.get("embedding")
        if not emb:
            continue
        score = _cosine_similarity(q_emb, emb)
        scored.append((score, e))

    if not scored:
        return "Your knowledge index seems empty or corrupted, sir."

    scored.sort(key=lambda x: x[0], reverse=True)
    top_k = scored[:4]

    # Build context for GPT
    context_parts = []
    for i, (score, e) in enumerate(top_k, start=1):
        src = os.path.basename(e["source"])
        snippet = e["text"].strip()
        context_parts.append(f"[Source {i} - {src}]\n{snippet}\n")
    context = "\n\n".join(context_parts)

    # We will not call GPT from here (thatâ€™s done in main.py via ask_gpt).
    # Instead we return the raw context + question, so main.py can feed it to GPT if needed.
    # But your current design already calls GPT directly from here, so we keep that.

    system_prompt = (
        "You are JARVIS, an AI assistant that answers questions ONLY using the user's personal notes. "
        "You are given some context extracted from their PDFs/notes. "
        "If the context does not contain the answer, say you do not know from their notes, "
        "and DO NOT invent extra facts."
    )

    user_prompt = f"CONTEXT:\n{context}\n\nQUESTION: {question}"

    if openai_client is None:
        # Fallback: no GPT available, just return the top snippets.
        return (
            "Here is what your notes say, sir (shortened because my GPT brain is offline):\n\n"
            + context
        )

    # Use GPT via the provided client (same as before)
    try:
        resp = openai_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=300,
        )
        ans = resp.choices[0].message.content.strip()
        return ans
    except Exception as e:
        print("[KB] GPT error:", e)
        return (
            "I tried to answer from your notes, but my thinking module failed, sir. "
            "Here is the raw context I found:\n\n" + context
        )
